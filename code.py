# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cn2683EBwWEb8vpJDee0h65tUOGTh0QR
"""

!pip install gensim
import gensim
from gensim.models import KeyedVectors
import csv


from google.colab import drive
drive.mount('/content/drive')

import os


model_location = '/content/drive/MyDrive/GoogleNews-vectors-negative300/data.bin.gz'

# Check if the file exists
if os.path.exists(model_location):
    print("File found!")
else:
    print("File not found. Check the file path.")

from google.colab import drive
drive.mount('/content/drive')

# Replace with the path to your .bin.gz file in Google Drive
compressed_model_location = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'

# Uncompress the .bin.gz file to get the .bin file
!gunzip {compressed_model_location}

# The path to the uncompressed .bin Word2Vec model
model_location = compressed_model_location.rstrip('.gz')

# Now you can load the model using Gensim
from gensim.models import KeyedVectors
wv = KeyedVectors.load_word2vec_format(model_location, binary=True, limit=1000000)

# Load the first million word vectors
wv = KeyedVectors.load_word2vec_format(model_location, binary=True, limit=1000000)

# Path to the CSV file where we want to save the vectors
csv_file_path = 'vectors.csv'

# Save the vectors to a CSV file
with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:
    csv_writer = csv.writer(csv_file, delimiter=',')
    # Write the header
    csv_writer.writerow(['word'] + ['dim_{}'.format(i) for i in range(wv.vector_size)])

    # Write each word and its vector
    for word in wv.index_to_key[:1000000]:  # Ensure we only write the first million
        csv_writer.writerow([word] + list(wv[word]))

import pandas as pd
import numpy as np
from scipy.spatial.distance import cosine

# Load the vectors CSV file into a pandas DataFrame
df = pd.read_csv('vectors.csv')

# Convert the DataFrame into a dictionary for faster lookup
word_vectors = {word: np.array(vector) for word, *vector in df.values}

# Define a function to compute the vector of a phrase
def phrase_to_vec(phrase):
    words = phrase.split()
    word_vecs = [word_vectors[word] for word in words if word in word_vectors]
    # Compute the mean vector of all word vectors in the phrase
    return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(df.shape[1] - 1)

# Define a function to calculate the semantic distance between two phrases
def semantic_distance(phrase1, phrase2):
    vec1 = phrase_to_vec(phrase1)
    vec2 = phrase_to_vec(phrase2)
    return cosine(vec1, vec2)

# Example usage:
phrase1 = "king"
phrase2 = "queen"
distance = semantic_distance(phrase1, phrase2)
print(f"The semantic distance between '{phrase1}' and '{phrase2}' is: {distance}")